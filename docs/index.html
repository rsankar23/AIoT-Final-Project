<!DOCTYPE HTML>
<!--
	Prologue by HTML5 UP
	html5up.net | @n33co
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Columbia University EECS E4764 IoT Project Report #0</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<script type="text/javascript" id="MathJax-script" async
      		src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    	</script>
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
	</head>
	<body>

		<!-- Header -->
			<div id="header">

				<div class="top">

					<!-- Logo -->
						<div id="logo">
							<!-- <span class="image avatar48"><img src="images/avatar.jpg" alt="" /></span> -->
							<h1 id="title">Vision Based Mobile Assistive Detection System</h1>
							<p>Columbia University <br>
								EECS E4764 Fall'25 <br>
								Artificial Intelligence of Things<br>
								Team 20 Project Report
							</p>
						</div>

					<!-- Nav -->
						<nav id="nav">
							<!--

								Prologue's nav expects links in one of two formats:

								1. Hash link (scrolls to a different section within the page)

								   <li><a href="#foobar" id="foobar-link" class="icon fa-whatever-icon-you-want skel-layers-ignoreHref"><span class="label">Foobar</span></a></li>

								2. Standard link (sends the user to another page/site)

								   <li><a href="http://foobar.tld" id="foobar-link" class="icon fa-whatever-icon-you-want"><span class="label">Foobar</span></a></li>

							-->
							<ul>
								<li><a href="#top" id="top-link" class="skel-layers-ignoreHref"><span class="icon fa-home">Abstract</span></a></li>
								<li><a href="#motivation" id="motivation-link" class="skel-layers-ignoreHref"><span class="icon fa-th">Motivation</span></a></li>
								<li><a href="#system" id="system-link" class="skel-layers-ignoreHref"><span class="icon fa-th">System</span></a></li>
								<li><a href="#results" id="results-link" class="skel-layers-ignoreHref"><span class="icon fa-th">Results</span></a></li>
								<li><a href="#references" id="references-link" class="skel-layers-ignoreHref"><span class="icon fa-th">References</span></a></li>
								<li><a href="#team" id="team-link" class="skel-layers-ignoreHref"><span class="icon fa-user">Our Team</span></a></li>
								<li><a href="#contact" id="contact-link" class="skel-layers-ignoreHref"><span class="icon fa-envelope">Contact</span></a></li>
							</ul>
						</nav>

				</div>

				<div class="bottom">

					<!-- Social Icons -->
						<ul class="icons">
							<li><a href="#" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
							<li><a href="#" class="icon fa-facebook"><span class="label">Facebook</span></a></li>
							<li><a href="#" class="icon fa-github"><span class="label">Github</span></a></li>
							<li><a href="#" class="icon fa-dribbble"><span class="label">Dribbble</span></a></li>
							<li><a href="#" class="icon fa-envelope"><span class="label">Email</span></a></li>
						</ul>

				</div>

			</div>

		<!-- Main -->
			<div id="main">

				<!-- Intro -->
					<section id="top" class="one dark cover">
						<div class="container">

								<iframe width="560" height="315" src="tps://youtu.be/6xA6abjEwww" frameborder="0" allowfullscreen></iframe>

								<h2 class="alt">Vision Based Mobile Assistive Detection System</h2>
								<p>Group 20 - Adib Khondoker, Dexin Huang, Revath Sankar</p>
								


							<footer>
								<a href="#motivation" class="button scrolly">Motivation</a>
							</footer>

						</div>
					</section>

				<!-- Portfolio -->
					<section id="motivation" class="two">
						<div class="container">

							<header>
								<h2>Motivation</h2>
							</header>
							<ul>
								<li>Goal of the project is to develop a computer vision system for local assistive operations by utilizing computer vision to support the visually impaired in low-light or poor-visibility conditions</li>
								<li>Use onboard detection hardware to locate other individuals and objects and relay their positions to a GUI on a laptop/mobile device for the user</li>
							</ul>
						</div>
					</section>


					<section id="system" class="three">
						<div class="container">

							<header>
								<h2>System</h2>
							</header>

							<p align="left">Remember to use combination of descriptions, photos, and figures</p>

							<h3 align="left">Architecture</h3>

							<figure style="text-align: center;">
  								<img src="images/sys_arch.png" style="max-width: 90%;">
  								<figcaption>Fig.0 – System Architecture</figcaption>
							</figure>

							<p align="left">
							The system follows an edge-to-cloud architecture designed to 
							support real-time assistive object detection. A Raspberry Pi equipped with a camera serves as the edge device, capturing image frames from the 
							environment. These frames are periodically transmitted via HTTPS POST requests to a YOLOv8 inference service deployed on Google Cloud Run. 
							The cloud service performs object detection and returns structured JSON responses containing class labels, confidence scores, and bounding box 
							coordinates. The Raspberry Pi then uses this information to render detections on a local graphical interface, providing situational awareness to the 
							user. While the architecture includes a flight control subsystem to illustrate a potential mobile deployment, autonomous navigation is presented as a 
							planned extension rather than a fully implemented feature in the current prototype.
							</p>


							<h3 align="left">Technical Components</h3>


							<h4 align="left">Real-Time Vision Algorithm</h4>
							<p align="left">
								The "YOLO" (You Only Look Once) algorithm is a prominent real-time object detection algorithm that uses a single convolutional neural network to predict bounding boxes and classify objects simultaneously. YOLO is unique in that it treats object detection as a single regression task, predicting bouding boxes and class probabilities directly from full images in one evaluation. This has made YOLO significantly faster than previous two-way detectors.
							</p>
							<figure>
								<img src = "images/yolo_v8_arch.webp" >
								<figcaption>Fig.1 - YOLO Backbone, Neck, Head Architecture [1]</figcaption>
							</figure>
							
							<h5 align="left"left>Model Architecture</h5>
							<p align="left">The YOLO model is comprised of a three-part architectured that follows a "Backbone, Neck, Head" pattern. In this architecture, the backbone serves to extract core features from the input image, the neck takes in multi-scale features from the backbone and fuses or aggregates them to create a richer feature set; often using the Feature Pyramid Network and Path Aggression Network techniques, finally, the head consumes the refined feature information from the neck to make final predictions, generate bounding box coordinates; object confidence scores and class labels. The model selects the best optimizer; in this case the ADAM Optimization algorithm. Additionally, the YOLO version 8, which we are utilizing for the purposes of this experiment leverages a modified CSP Darknet architecture - with several C2F modules to reduce computational redundancy, improve gradient flow and feature reuse; the model is also anchor-free, allowing the model to directly predict bounding box properties, eliminating the need for pre-defined anchor boxes and the typical clustering process.</p>

							<table style="border: 2px solid; border-color: #114D34;">
								<tr style="border: 2px solid #114D34;">
									<th><strong>Parameter</strong></th>
									<th><b>Value</b></th>
								</tr>
								<tr>
									<th>Training Epochs</th>
									<th>100</th>
								</tr>
								<tr>
									<th>Image Size</th>
									<th>640 px</th>
								</tr>
								<tr>
									<th>Training Size</th>
									<th>80%</th>
								</tr>
								<tr>
									<th>Cosine Learning Rate</th>
									<th>True</th>
								</tr>
								<tr>
									<th>Workers</th>
									<th>100</th>
								</tr>
								<th>Learning Rate</th>
								<th>1e-2</th>
							</table>
						<hr>
						<h5 align="left">HomeObjects-3K Dataset</h5>
							<p align="left">The HomeObjects-3K dataset is a curated collection of indoor household object images designed specifically for training, validating, and benchmarking object detection models like YOLO. It was released by the Ultralytics team (the maintainers of YOLO) and is included as a sample dataset in the Ultralytics documentation and tools.[2] We chose this dataset because of our focus in assisted living environments and low-light indoor environments and identification of common household objects in those environments.</p>
							<table style="border: 2px solid; border-color: #114D34;">
								<tr style="border: 2px solid; border-color: #114D34;">
									<th>Total Images</th>
									<th>Training Set Size</th>
									<th>Validation Set Size</th>
								</tr>
								<tr>
									<td>~3,000</td>
									<td>2,285</td>
									<td>404</td>
								</tr>
							</table>

							<table>
								<tr>
									<th>True Label</th>
									<th>Predictions</th>
								</tr>
								<tr>
									<td><img src="images/data_snap/val_batch0_labels.jpg" class="plot"></td>
									<td><img src="images/data_snap/val_batch0_pred.jpg" class="plot"></td>
								</tr>
							</table>

						<hr>
						<h4 align="left">Cloud API</h4>
							<p align="left">
								YOLO model deployed on Google Cloud Run. Accepts JPEG via HTTPS POST. Returns JSON with detections, confidence, and bounding boxes. Latency is 150ms per inference.
							</p>

							<h5 align="left">Live Endpoint</h5>
							<p align="left"><strong>Base URL:</strong> <code>https://uav-rescue-detection-xhp2lso5pq-uc.a.run.app</code></p>

							<table style="border: 2px solid; border-color: #114D34;">
								<tr style="border: 2px solid #114D34;">
									<th>Endpoint</th>
									<th>Method</th>
									<th>Description</th>
								</tr>
								<tr>
									<td><code>/health</code></td>
									<td>GET</td>
									<td>Health check</td>
								</tr>
								<tr>
									<td><code>/classes</code></td>
									<td>GET</td>
									<td>List detectable classes</td>
								</tr>
								<tr>
									<td><code>/detect</code></td>
									<td>POST</td>
									<td>Run YOLO inference</td>
								</tr>
							</table>

							<h5 align="left">Response Format</h5>
							<pre align="left" style="background: #f4f4f4; padding: 10px; text-align: left;">
{
  "status": "success",
  "person_detected": true,
  "num_detections": 2,
  "detections": [
    {
      "class": "Person",
      "confidence": 0.9878,
      "bbox": [120.5, 45.2, 380.1, 520.8]
    }
  ],
  "latency_ms": 145.32
}
							</pre>

							<h5 align="left">System Architecture</h5>
							<pre align="left" style="background: #f4f4f4; padding: 10px; text-align: left;">
ESP32-CAM --> HTTPS POST --> Cloud Run (YOLOv8) --> JSON Response
    |                             |                      |
  Capture                    Inference            person_detected
  Image                      ~150ms                  true/false
							</pre>
						<p align="left">CAM captures images and sends to Cloud Run API. API returns detection results for display or alerts.</p>
						<hr>
						<h3 align="left">Prototype</h3>

					

						<figure style="text-align: center;">
  								<img src="images/circuit_dgm.png" style="max-width: 90%;">
  								<figcaption>Wiring Diagram of peripherals</figcaption>
						</figure>

						<hr>
						<h4 align="left">Prototype - Sensors & Peripherals</h4>
						
						<p align="left">
								For our Prototype Autonomous Drone system, the components consisted of a Raspberry PI + Cam for primary flight control and image capture, 4 Lidar Sensors, a BNO055 IMU, and a PWM breakout board on a 3rd party drone kit package
								The goal was to utilize lidar sensors covering the drones sides to keep it from hitting into obstacles, an Adafruit BNO055 IMU to keep track of its position and how far it has traveled
								and feed it into a PID algorithm to keep the drones flight characteristics stable. Then the onboard camera would capture images
						</p>

						
				

						<p align="left">
						Autonomous mobility was explored as part of the system design; however, full autonomous flight was not completed within the project timeframe. During testing, a hardware failure resulted in a crash that prevented further validation of the autonomous control loop. As a result, the implemented prototype focuses on the vision-based detection, cloud inference, and visualization pipeline, while the flight-control subsystem is presented as planned future work.
						</p>

						<figure style="text-align: center;">
  								<img src="images/bn005imu.png" style="max-width: 90%;">
  								<figcaption>Adafruit BNO005 IMU</figcaption>
						</figure>

						<p align="left">
						The Adafruit BNO055 serves as the inertial sensing component of the system, providing real-time orientation and motion data. It is a 9-DOF absolute orientation sensor that integrates an accelerometer, gyroscope, and magnetometer and communicates with the raspberry pi over UART. 
						Within the system architecture, the BNO055 is intended to supply attitude and orientation feedback to the flight-control subsystem. In a fully autonomous configuration, this data would be combined with vision-based detections and other sensor inputs and fed into a PID control loop to stabilize motion and guide navigation. 
						To use this sensor with the Raspberry PI, the following library must be installed using: <br> 

						sudo pip3 install adafruit-circuitpython-bno055
						</p>

						<figure style="text-align: center;">
  								<img src="images/lidar.jpg" style="max-width: 90%;">
  								<figcaption>VL53L0X Lidar Sensors</figcaption>
						</figure>

						<p align="left">
						The VL53L0X LiDAR sensor is used for short-range distance measurements with a typical sensing range of up to approximately 2 meters according to its specifications.
						In the proposed system, multiple VL53L0X sensors would be mounted on the sides of the drone to monitor proximity to surrounding surfaces 
						such as walls and furniture to allow the system to detect obstacles before physical contact occurs, enabling the drone to slow down 
						or adjust its trajectory to avoid collisions during operation.
						</p>

						<p align="left">
						To use these sensors with the Raspberry pi, you must install these libraries and enable 12c communication through:
						</p>

						<p align="left">
						sudo pip3 install adafruit-circuitpython-vl53l0x

						</p>

						<p align="left">
						sudo apt-get install python3-smbus
						</p>

						<p align="left">
						Enable I2C Interface:

						sudo raspi-config
						# Navigate to: Interface Options → I2C → Enable <br>
						</p>

						<p align="left">
						Because multiple VL53L0X LiDAR sensors share the same default I²C address, an address management procedure is required to operate multiple sensors on a single I²C bus. Each sensor exposes an XSHUT (shutdown) pin, which allows the device to be individually enabled or disabled. On system boot, all sensors are initially held in shutdown. Sensors are then enabled one at a time, and upon activation, each sensor’s I²C address is reassigned in software before enabling the next sensor. This process ensures that all sensors operate concurrently on the same bus without address conflicts. The address assignment and sensor initialization are implemented programmatically using Python on the Raspberry Pi.
						</p>

						<figure style="text-align: center;">
  								<img src="images/pwm.jpg" style="max-width: 90%;">
  								<figcaption>Adafruit 16 channel PWM PCA9685</figcaption>
						</figure>

						<p align="left">
						The Adafruit 16-Channel PWM/Servo HAT (PCA9685) is used to generate stable, hardware-timed PWM signals needed for motor and actuator control. 
						The Raspberry Pi has limited native hardware PWM outputs, and software-driven PWM can introduce timing jitter when the CPU is busy 
						(capturing frames, networking, or running application logic). 
						The PWM generation offloaded to a dedicated controller over I2C. 
						In our system design, this board would regulate motor thrust and enable consistent control 
						behavior required for closed-loop stabilization. Although full autonomous flight was not completed within the project timeframe, the PWM HAT 
						defines the intended motor-control layer for scalable, reliable actuation. <br>

						You need to install the following libraries<br>

    					sudo pip3 install adafruit-circuitpython-pca9685 <br>
						sudo pip3 install adafruit-circuitpython-servokit

						</p>

						<hr>
						<h4 align="left">Prototype - PID Algorithm</h4>
						
						<figure style="text-align: center;">
  								<img src="images/PID_Controller.png" style="max-width: 90%;">
  								<figcaption>Diagram of PID Controller Algorithm by Ariffanan Basri </figcaption>
						</figure>
						
						<h4 align="left">Planned PID-Based Control Loop (Intended Autonomous Behavior)</h4>

						<p align="left">
						To support autonomous mobility, we designed a closed-loop PID control structure that would continuously correct the drone’s motion using sensor feedback. At a high level, the controller compares a desired setpoint (e.g., target altitude, heading, or position) against the measured state from onboard sensors, computes an error signal, and outputs motor commands that reduce this error over time. The standard PID control law is:
						</p>

						<p align="left">
						$$e(t) = x_d(t) - x(t)$$
						$$u(t) = K_p e(t) + K_i\int_0^t e(\tau)\,d\tau + K_d\frac{de(t)}{dt}$$
						</p>

						<p align="left">
						In our implementation plan, the proportional term would provide immediate corrective action proportional to the current error, the integral term would compensate for persistent bias (e.g., gravity, imbalance, steady disturbances), and the derivative term would damp rapid changes to reduce overshoot and oscillation. Because quadrotor dynamics are coupled and underactuated, the intended design uses a cascaded approach rather than a single PID loop for all states.
						</p>

						<ul align="left">
						<li><b>Altitude control:</b> a PID loop on vertical position would output a total thrust command (collective thrust) to raise or lower the drone.</li>
						<li><b>Attitude control:</b> separate PID loops for roll, pitch, and yaw would output corrective torque commands to stabilize the platform orientation.</li>
						<li><b>Position control:</b> outer-loop PID control on horizontal position (x/y) would generate desired roll and pitch setpoints (tilt commands). These attitude setpoints would then be tracked by the inner attitude PID loops.</li>
						<li><b>Motor mixing:</b> the final stage would convert the collective thrust and torque commands into individual motor/ESC PWM signals (e.g., via a PCA9685 PWM HAT), enabling differential thrust for stabilization and turning.</li>
						</ul>

						<p align="left">
						In practice, this structure would run at a fixed control frequency with measured timestep <i>dt</i>, include actuator saturation limits, and implement anti-windup handling for the integral term to prevent instability when motor commands hit their maximum or minimum values. While full autonomous flight was not completed within the project timeframe due to hardware failure during testing, the above control design reflects the intended approach we aimed to implement and tune on our platform.
						</p>
						
						<hr>
						<h4 align="left">System Operation and Flight Software Architecture</h4>
						
						<p align="left">
During flight, the onboard software is structured as a dual-loop system to ensure that time-critical motor control is never blocked by network operations. A high-frequency control loop continuously reads orientation data from the BNO055 inertial measurement unit (IMU) over the I<sup>2</sup>C bus to estimate the current attitude of the platform (roll, pitch, and yaw). This loop computes stabilization corrections using a PID controller and sends updated motor commands to the PCA9685 PWM HAT, which generates PWM signals for each electronic speed controller (ESC).
</p>

<p align="left">
In parallel, a lower-frequency perception loop periodically captures a JPEG image from the Raspberry Pi camera and transmits it to a Google Cloud Run endpoint via an HTTPS POST request. The server performs YOLO-based object detection and returns a JSON response containing detection results such as class labels, confidence scores, and bounding box coordinates. These results are stored in shared state and can be accessed by higher-level mission logic.
</p>

<p align="left">
Based on the returned perception data, the mission logic can optionally modify behavior, such as pausing translational motion and maintaining a stable hover when a person is detected. For demonstration purposes, the overall mission profile is implemented as a predetermined, time-based sequence (e.g., takeoff and hover, small yaw sweeps, and forward motion segments), while the perception pipeline runs concurrently in the background and logs detections in real time.
</p>
						

						</div>
					</section>


					<section id="results" class="two">
						<div class="container">

							<header>
								<h2>Results</h2>
							</header>
							<h4 align="left">YOLO Object Detection Performance</h4>
							<p align="left">Fine-tuning YOLOv8n on home-relevant classes achieved 85.4% mAP with significant noise reduction. By focusing on only 12 classes of common household objects instead of 80, the model eliminates false positives like surfboards and laptops that distract from assistive operations.[3]</p>
							<p align="left">$$Precision = \frac{TP}{TP + FP}$$</p>
							<p align="left">$$Recall = \frac{TP}{TP + FN}$$</p>
							<p align="left">$$F1 = 2\times\frac{precision \times recall}{precision + recall}$$</p>
							<p align="left">Below are the plots generated from the validation phase with a randomly sampled 20% of the initial dataset. The F1 Confidence Curve plots the F1 score against different confidence thresholds. A higher F1 score indicates better performance, and the confidence threshold at which the F1 score is maximized is often considered the optimal threshold for making predictions; in the pre-trained model, 0.345 for all classes, and in the case of the base model, we cannot identify such a point across any class. As seen in Fig.2, the F1 Confidence curve under the pre-trained model demonstrates a higher peak when managing the HomeObjects-3K dataset, compared to the base model @ (0.345, 0.67) for the totality of classes. We can next evaluate the Precision-Recall Curve, which is a plot that shows the trade-off between precision and recall for different threshold values. The area under the curve (AUC) is a measure of how well the model is able to distinguish between classes; and as expected the pretrained model not only exhibits a higher AUC (0.676), but also has does not exhibit a near-zero area unlike the base model (0.019); this essentially confirms that the initial training is insufficient for our task of identification and detection. The Precision-Confidence & Recall-Confidence Curve plots precision and recall respectively against different confidence thresholds. For both of these cases, we can see that the Non-Pretrained model significantly underpreformed, not exhibitng the typical or ideal behavior in either case (Recall-Confidence with trendline decreasing from top-left to bottom-right traversing along x-axis; Precision-Confidence having a trendline that exhibits an increasing precision along the x-axis with high confidence across all precisions). Similarly within the Precision-Confidence curve, many of the classes reach the ideal "1.0" threshold only after traversing several confidence levels. Displaying an opposite but parallel behavior is the Recall-Confidence curve, where the pre-trained model exhibits a smooth downward curve, the base model essentially stays flat near-zero across all relevant classes.</p>
							<p align="left">On review of the summary statistics, we can see that for each of the twelve classes the measured precision (P), recall (R), mean average precision (mAP50) commonly used to evaluate the quality of object detectors, and Mean Average Precision averaged over IoU thresholds from 50% to 95% ( a stricter object detection evaluation metric), the pretrained model consistently outperforms the base model in bounding and detection tasks. The precision and recall of the object classifications and bounding box measurements are orders of magnitude higher in the pre-trained model as compared to the base model. This is further exemplified through the Confusion matrices where the non-pretrained model not only commonly misidentifies common household objects, but also misidentifies objects as background content.</p>
							<table style="border: 2px solid; border-color: #114D34;">
								<tr style="border: 2px solid; border-color: #114D34;">
									<td>Base Model Summary Statistics</td>
									<td><img src="images/summary_stats/Screenshot 2025-12-11 at 7.56.14 PM.png"</td>
								</tr>
								<tr>
									<td>Pre-Trained Model Summary Statistics</td>
									<td><img src="images/summary_stats/Screenshot 2025-12-11 at 7.55.50 PM.png"</td>
								</tr>
							</table>

							<hr>
							<ol align="left" type="I">
								<li>True Positives (TP): The cases in which the model correctly predicted the positive class.</li>
								<li>True Negatives (TN): The cases in which the model correctly predicted the negative class.</li>
								<li> Positives (FP): The cases in which the model incorrectly predicted the positive class (a "false alarm").</li>
								<li>False Negatives (FN): The cases in which the model failed to predict the positive class (a "miss").</li>
							</ol>

							<hr>
							<table style="border: 2px solid; border-color: #114D34;">
								<tr style="border: 2px solid; border-color: #114D34;">
									<th>Training Statistic</th>
									<th>Value</th>
								</tr>
								<tr>
									<td>Box Loss</td>
									<td>0.8619</td>
								</tr>
								<tr>
									<td>Classification Loss</td>
									<td>0.5749</td>
								</tr>
								<tr>
									<td>Focal Loss (DFL)</td>
									<td>1.004</td>
								</tr>
							</table>
							<figure>
							<table style="border: 2px solid; border-color: #114D34;" class="plots">
								<tr style="border: 2px solid; border-color: #114D34;">
									<th>Metric</th>
									<th>Non-Pretrained Model Plot</th>
									<th>Pretrained Model Plot</th>
								</tr>
								<tr >
									<td>F1-Confidence Curve</td>
									<td><img src="images/no_pre/BoxF1_curve.png" class="plot"></td>
									<td><img src="images/BoxF1_curve_1.png" class="plot"></td>
								</tr>
								<tr>
									<td>Precision-Confidence Curve</td>
									<td><img src="images/no_pre/BoxP_curve.png" class="plot"></td>
									<td><img src="images/BoxP_curve.png" class="plot"></td>
								</tr>
								<tr>
									<td>Precision-Recall Curve</td>
									<td><img src="images/no_pre/BoxPR_curve.png" class="plot"></td>
									<td><img src="images/BoxPR_curve.png" class="plot"></td>
								</tr>
								<tr>
									<td>Recall-Confidence Curve</td>
									<td><img src="images/no_pre/BoxR_curve.png" class="plot"></td>
									<td><img src="images/BoxR_curve.png" class="plot"></td>
								</tr>
								<tr>
									<td>Confusion Matrix</td>
									<td><img src="images/no_pre/confusion_matrix_normalized.png" class="plot"></td>
									<td><img src="images/confusion_matrix_normalized.png" class="plot"></td>
								</tr>
							</table>
							<figcaption>Figure 2 - Validation Set Performance for Non-Pretrainefd and Pretrained YOLOv8 Models on HomeObjects-3K dataset</figcaption>
							</figure>
					


						</div>
					</section>

					<section id="references" class="three">
						<div class="container">

							<header>
								<h2>References</h2>
							</header>
							<ol>
								<li>
									<blockquote cite="https://viso.ai/deep-learning/yolov8-guide/">
									<p>Boesch, Gaudenz. “Explore Yolov8: Latest in Object Detection Tech.” Viso.Ai, viso.ai, 4 Apr. 2025, viso.ai/deep-learning/yolov8-guide/. </p>
									</blockquote>
								</li>
								<li>
									<blockquote cite="https://docs.ultralytics.com/datasets/detect/homeobjects-3k/">
									<p>Ultralytics. “HomeObjects-3K Dataset.” Ultralytics YOLO Docs, 16 Nov. 2025, docs.ultralytics.com/datasets/detect/homeobjects-3k/. </p>
									</blockquote>
								</li>
								<li>
									<blockquote cite="https://docs.ultralytics.com/models/yolov8/">
									<p>Ultralytics. “Explore Ultralytics Yolov8.” Ultralytics YOLO Docs, 28 Oct. 2025, docs.ultralytics.com/models/yolov8/. </p>
									</blockquote>
								</li>
								<li>
									<blockquote cite="https://www.researchgate.net/publication/373457893_Trajectory_Tracking_of_a_Quadcopter_UAV_using_PID_Controller/">
									<p>Basri, Ariffanan . “Trajectory Tracking of a Quadcopter UAV using PID Controller </p>
									</blockquote>
								</li>
							</ol>
							

						</div>
					</section>


				<!-- About Me -->
					<section id="team" class="two">
						<div class="container">

							<header>
								<h2>Our Team</h2>
							</header>

							<!-- <a href="#" class="image featured"><img src="images/pic08.jpg" alt="" /></a> -->


							<div class="row">
								<div class="4u 12u$(mobile)">
									<article class="item">
										<a href="#" class="image fit"><img src="images/RS_Profile.jpeg" alt="" /></a>
										<header>
											<h3>Revath Sankar</h3>
											<p>Masters candidate in Biomedical Engineering - Interested in applications of computer vision in biomedical AI.</p>
											<a href="https://www.linkedin.com/in/revath-sankar/">LinkedIn</a>
										</header>
									</article>
								</div>
								<div class="4u 12u$(mobile)">
									<article class="item">
										<a href="#" class="image fit"><img src="images/Adib_pfp.jpg" alt="" /></a>
										<header>
											<h3>Adib Khondoker</h3>
											<p>Masters Candidate in Computer Engineering - Interested in Embedded Systems and FPGA Design</p>
											<p>aak2250@columbia.edu</p>
										</header>
									</article>
								</div>
								<div class="4u$ 12u$(mobile)">
									<article class="item">
										<a href="#" class="image fit"><img src="images/pic07.jpg" alt="" /></a>
										<header>
											<h3>Dexin Huang</h3>
											<p>dh3172@columbia.edu</p>
										</header>
									</article>
								</div>
							</div>

							

						</div>
					</section>

				<!-- Contact -->
					<section id="contact" class="four">
						<div class="container">

							<header>
								<h2>Contact</h2>
							</header>

							<p align="left">
								<strong>Revath Sankar: </strong>rs4485@columbia.edu</br>
								<strong>Dexin Huang: </strong>dh3172@columbia.edu</br>
								<strong>Adib Khondoker: </strong>aak2250@columbia.edu</br>
							</br>
								<strong>Columbia University </strong><a href="http://www.ee.columbia.edu">Department of Electrical Engineering</a><br>
								<!-- <strong>Class Website:</strong>
									<a href="https://edblogs.columbia.edu/eecs4764-001-2019-3/">Columbia University EECS E4764 Fall '22 IoT</a></br> -->
								<strong>Instructor:</strong> <a href="https://www.engineering.columbia.edu/faculty-staff/directory/xiaofan-fred-jiang">Professsor Xiaofan (Fred) Jiang</a>
							</p>


							<!-- <form method="post" action="#">
								<div class="row">
									<div class="6u 12u$(mobile)"><input type="text" name="name" placeholder="Name" /></div>
									<div class="6u$ 12u$(mobile)"><input type="text" name="email" placeholder="Email" /></div>
									<div class="12u$">
										<textarea name="message" placeholder="Message"></textarea>
									</div>
									<div class="12u$">
										<input type="submit" value="Send Message" />
									</div>
								</div>
							</form> -->

						</div>
					</section>

			</div>

		<!-- Footer -->
			<div id="footer">

				<!-- Copyright -->
					<ul class="copyright">
						<li>&copy; IoT Project | All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollzer.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

	</body>
</html>
